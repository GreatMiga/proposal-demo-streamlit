# app.py
"""
Streamlit demo app:
"Ø³Ø§Ù…Ø§Ù†Ù‡ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù…Ù‚Ø§Ù„Ø§Øª Ù…Ø´Ø§Ø¨Ù‡ / Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ø¯Ø§ÙˆØ± (Ø¯Ù…ÙˆÛŒ Ù¾Ø±ÙˆÙ¾ÙˆØ²Ø§Ù„)"

Ù†Ú©Ø§Øª:
- Ø§ÛŒÙ† Ù†Ø³Ø®Ù‡ Ø¨Ø±Ø§ÛŒ Ø¯Ù…ÙˆÛŒ Ù¾Ø±ÙˆÙ¾ÙˆØ²Ø§Ù„ Ø·Ø±Ø§Ø­ÛŒ Ø´Ø¯Ù‡: Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ø² Semantic Scholar Ú¯Ø±ÙØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯.
- Ø¯Ø± Ù†Ø³Ø®Ù‡ Ù†Ù‡Ø§ÛŒÛŒ ØªÙˆØµÛŒÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø§Ø² Scopus/PubMed Ùˆ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ domain-specific fine-tune Ø´Ø¯Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯.
"""

import time
import requests
from datetime import datetime
from typing import List, Dict, Any
import streamlit as st
import pandas as pd
from sentence_transformers import SentenceTransformer, util
import numpy as np
import plotly.express as px

# ---------------------------------------------------------------------
# Config
# ---------------------------------------------------------------------
SEMANTIC_SCHOLAR_SEARCH = "https://api.semanticscholar.org/graph/v1/paper/search"
SEMANTIC_SCHOLAR_AUTHOR = "https://api.semanticscholar.org/graph/v1/author/{}"
SEARCH_LIMIT_DEFAULT = 50  # Ø¨Ø±Ø§ÛŒ Ø¯Ù…ÙˆÛŒ Ø§ÙˆÙ„ÛŒÙ‡
REQUEST_SLEEP = 0.2  # Ø§Ø­ØªØ±Ø§Ù… Ø¨Ù‡ rate-limit (ØªÙ†Ø¸ÛŒÙ… Ù‚Ø§Ø¨Ù„ ØªØºÛŒÛŒØ±)

# ---------------------------------------------------------------------
# Streamlit page
# ---------------------------------------------------------------------
st.set_page_config(page_title="Ø¯Ù…ÙˆÛŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ø¯Ø§ÙˆØ±/Ù…Ù‚Ø§Ù„Ø§Øª Ù…Ø´Ø§Ø¨Ù‡", layout="wide")
st.title("ğŸ§­ Ø³Ø§Ù…Ø§Ù†Ù‡ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù…Ù‚Ø§Ù„Ø§Øª Ù…Ø´Ø§Ø¨Ù‡ â€” Ø¯Ù…ÙˆÛŒ Ù¾Ø±ÙˆÙ¾ÙˆØ²Ø§Ù„")
st.markdown(
    """
Ø§ÛŒÙ† Ù†Ø³Ø®Ù‡â€ŒÛŒ Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø² **Sentence Transformers** Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù…Ù‚Ø§Ù„Ø§Øª Ù…Ø´Ø§Ø¨Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ø² Semantic Scholar Ú¯Ø±ÙØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ (Ø¨Ø±Ø§ÛŒ Ø¯Ù…ÙˆÛŒ Ø§ÙˆÙ„ÛŒÙ‡). Ø¯Ø± Ù†Ø³Ø®Ù‡â€ŒÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø§Ø² **Scopus API** Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø®ÙˆØ§Ù‡Ø¯ Ø´Ø¯.
"""
)

# ---------------------------------------------------------------------
# Helpers: Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ùˆ API calls
# ---------------------------------------------------------------------
@st.cache_resource(show_spinner=False)
def load_bi_encoder(prefer_specter: bool = True):
    """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ bi-encoder (Ø³Ø¹ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ SPECTER Ø±Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†Ø¯Ø› Ø¯Ø± ØµÙˆØ±Øª Ø¹Ø¯Ù…â€ŒØ¯Ø³ØªØ±Ø³ÛŒ Ø§Ø² MPNet Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯)."""
    tried = []
    if prefer_specter:
        try:
            st.info("Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ SPECTER (Ø®ØµÙˆØµÛŒØªÙ‡ Ù…Ù‚Ø§Ù„Ø§Øª Ø¹Ù„Ù…ÛŒ)...")
            model = SentenceTransformer("allenai-specter")
            return model
        except Exception as e:
            tried.append(("allenai-specter", str(e)))

    # fallback
    try:
        st.info("Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ all-mpnet-base-v2 (ÙÙˆÙ„ Ø¨Ú©).")
        model = SentenceTransformer("sentence-transformers/all-mpnet-base-v2")
        return model
    except Exception as e:
        tried.append(("all-mpnet-base-v2", str(e)))
        # last fallback to MiniLM
    try:
        st.info("Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ all-MiniLM-L6-v2 (Ú©Ù…â€ŒØ­Ø¬Ù…).")
        model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
        return model
    except Exception as e:
        tried.append(("all-MiniLM-L6-v2", str(e)))
        raise RuntimeError(f"Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ø´Ú©Ø³Øª Ø®ÙˆØ±Ø¯. ØªÙ„Ø§Ø´â€ŒÙ‡Ø§ÛŒ Ø§Ù†Ø¬Ø§Ù…â€ŒØ´Ø¯Ù‡: {tried}")

@st.cache_resource(show_spinner=False)
def load_reranker():
    """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ cross-encoder reranker Ø§Ø®ØªÛŒØ§Ø±ÛŒ (Ø³Ø¨Ú©)"""
    try:
        from sentence_transformers import CrossEncoder
        # Ù…Ø¯Ù„ Ø³Ø±ÛŒØ¹ Ùˆ Ø³Ø¨Ú© MS-MARCO style Ø¨Ø±Ø§ÛŒ reranking
        reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")
        return reranker
    except Exception as e:
        st.warning("Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ reranker Ù…ÙˆÙÙ‚ Ù†Ø¨ÙˆØ¯Ø› reranker ØºÛŒØ±ÙØ¹Ø§Ù„ Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
        return None

@st.cache_data(ttl=3600)
def semantic_scholar_search(query: str, limit: int = 50) -> List[Dict[str, Any]]:
    """Ø¬Ø³ØªØ¬ÙˆÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø¯Ø± Semantic Scholar (metadata)"""
    params = {
        "query": query,
        "limit": limit,
        "fields": "title,abstract,authors.name,authors.authorId,publicationDate,venue,url"
    }
    try:
        resp = requests.get(SEMANTIC_SCHOLAR_SEARCH, params=params, timeout=20)
        time.sleep(REQUEST_SLEEP)
        if resp.status_code == 200:
            return resp.json().get("data", [])
        else:
            st.error(f"Semantic Scholar API returned status {resp.status_code}")
            return []
    except requests.exceptions.RequestException as e:
        st.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ Semantic Scholar: {e}")
        return []

@st.cache_data(ttl=3600)
def get_author_profile(author_id: str) -> Dict[str, Any]:
    """Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø² Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨ÛŒØ´ØªØ± Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡ Ø±Ø§ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯ (h-index, paperCount)"""
    if not author_id:
        return {}
    try:
        resp = requests.get(SEMANTIC_SCHOLAR_AUTHOR.format(author_id), params={"fields": "name,hIndex,paperCount,affiliations"}, timeout=10)
        time.sleep(REQUEST_SLEEP)
        if resp.status_code == 200:
            return resp.json()
        return {}
    except requests.exceptions.RequestException:
        return {}

# ---------------------------------------------------------------------
# UI: ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ú©Ø§Ø±Ø¨Ø±
# ---------------------------------------------------------------------
with st.sidebar:
    st.header("ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¯Ù…ÙˆÛŒ Ù¾Ø±ÙˆÙ¾ÙˆØ²Ø§Ù„")
    prefer_specter = st.checkbox("Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ SPECTER (Ø§Ú¯Ø± Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯)", value=True)
    use_reranker = st.checkbox("Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Reranker Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ù†ØªØ§ÛŒØ¬ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)", value=True)
    top_k = st.slider("ØªØ¹Ø¯Ø§Ø¯ Ù†ØªØ§ÛŒØ¬ Ø¨Ø§Ù„Ø§ÛŒÛŒ Ú©Ù‡ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ø´ÙˆØ¯ (Top K)", min_value=5, max_value=30, value=10)
    search_limit = st.slider("Ø­Ø¯Ø§Ú©Ø«Ø± Ù…Ù‚Ø§Ù„Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ø§ÙˆÙ„ÛŒÙ‡ (Semantic Scholar)", min_value=10, max_value=100, value=SEARCH_LIMIT_DEFAULT)
    similarity_threshold = st.slider("Ø¢Ø³ØªØ§Ù†Ù‡ Ø´Ø¨Ø§Ù‡Øª Ø¨Ø±Ø§ÛŒ Ø´Ù…Ø§Ø±Ø´ 'Ù…Ù‚Ø§Ù„Ø§Øª Ù…Ø±ØªØ¨Ø·' (cosine)", 0.0, 1.0, 0.30, 0.01)
    st.markdown("---")
    st.caption("Ù†Ú©ØªÙ‡: Ø§ÛŒÙ† ÛŒÚ© Ù†Ø³Ø®Ù‡â€ŒÛŒ Ù†Ù…Ø§ÛŒØ´ÛŒ Ø§Ø³Øª. Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ø³Ø®Ù‡ Ù†Ù‡Ø§ÛŒÛŒ Ø§Ø² Scopus/PubMed Ú¯Ø±ÙØªÙ‡ Ø®ÙˆØ§Ù‡Ù†Ø¯ Ø´Ø¯.")

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§
model = load_bi_encoder(prefer_specter=prefer_specter)
reranker = load_reranker() if use_reranker else None

# ---------------------------------------------------------------------
# Main UI: ÙˆØ±ÙˆØ¯ÛŒ Ú©Ø§Ø±Ø¨Ø±
# ---------------------------------------------------------------------
st.subheader("Ú†Ú©ÛŒØ¯Ù‡ ÛŒØ§ Ø¹Ù†ÙˆØ§Ù† Ù¾Ø±ÙˆÙ¾ÙˆØ²Ø§Ù„ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯")
user_query = st.text_area("Ú†Ú©ÛŒØ¯Ù‡ ÛŒØ§ Ø¹Ù†ÙˆØ§Ù†", height=200, placeholder="Ù…Ø«Ø§Ù„: 'Machine learning methods for predicting drug solubility'")

col1, col2 = st.columns([1, 1])
with col1:
    run_button = st.button("ğŸ” Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù…Ù‚Ø§Ù„Ø§Øª Ù…Ø´Ø§Ø¨Ù‡")
with col2:
    st.write("")
    st.write("")

# ---------------------------------------------------------------------
# Main logic
# ---------------------------------------------------------------------
if run_button:
    if not user_query or user_query.strip() == "":
        st.warning("Ù„Ø·ÙØ§Ù‹ Ø§Ø¨ØªØ¯Ø§ ÛŒÚ© Ú†Ú©ÛŒØ¯Ù‡ ÛŒØ§ Ø¹Ù†ÙˆØ§Ù† ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯.")
    else:
        with st.spinner("Ø¯Ø± Ø­Ø§Ù„ Ø§Ø¬Ø±Ø§ â€” Ø¯Ø±ÛŒØ§ÙØª Ù…Ù‚Ø§Ù„Ø§Øª Ùˆ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øªâ€ŒÙ‡Ø§..."):
            # 1) Ø¬Ø³ØªØ¬ÙˆÛŒ Ø§ÙˆÙ„ÛŒÙ‡
            initial_papers = semantic_scholar_search(user_query, limit=search_limit)
            if not initial_papers:
                st.error("Ù…Ù‚Ø§Ù„Ù‡â€ŒØ§ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ ÛŒØ§ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§.")
            else:
                st.success(f"{len(initial_papers)} Ù…Ù‚Ø§Ù„Ù‡ Ø§Ø² Semantic Scholar Ø¯Ø±ÛŒØ§ÙØª Ø´Ø¯ (Ù†Ù…ÙˆÙ†Ù‡).")
                # 2) Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ùˆ Ù…ØªÙˆÙ† Ø¨Ø±Ø§ÛŒ embedding
                paper_texts = []
                paper_meta = []
                for p in initial_papers:
                    title = p.get("title") or ""
                    abstract = p.get("abstract") or ""
                    # Ø§Ú¯Ø± abstract Ù†ÛŒØ³Øª Ø§Ø² title Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
                    text_for_embed = abstract if abstract.strip() else title
                    # Ø¨Ù‡â€ŒØ¹Ù†ÙˆØ§Ù† snippet Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ØŒ ØªØ±Ø¬ÛŒØ­Ø§ abstract Ú©ÙˆØªØ§Ù‡
                    snippet = (abstract[:600] + "...") if abstract else title
                    paper_texts.append(text_for_embed)
                    paper_meta.append({
                        "paperId": p.get("paperId"),
                        "title": title,
                        "snippet": snippet,
                        "authors": p.get("authors", []),
                        "publicationDate": p.get("publicationDate"),
                        "venue": p.get("venue"),
                        "url": p.get("url")
                    })

                # 3) Ù…Ø­Ø§Ø³Ø¨Ù‡ embedding Ù‡Ø§ (batch)
                batch_size = 64
                paper_embeddings = model.encode(paper_texts, batch_size=batch_size, convert_to_tensor=True, show_progress_bar=False)
                query_embedding = model.encode(user_query, convert_to_tensor=True)

                # 4) Ù…Ø­Ø§Ø³Ø¨Ù‡ cosine similarities
                cosine_scores = util.cos_sim(query_embedding, paper_embeddings).cpu().numpy().flatten()

                # 5) assemble DataFrame
                df = pd.DataFrame(paper_meta)
                df["similarity"] = cosine_scores
                df = df.sort_values("similarity", ascending=False).reset_index(drop=True)

                # 6) (Ø§Ø®ØªÛŒØ§Ø±ÛŒ) rerank Ú©Ø±Ø¯Ù† top_n Ø¨Ø§ cross-encoder Ø¨Ø±Ø§ÛŒ Ø¯Ù‚Øª Ø¨ÛŒØ´ØªØ±
                if reranker is not None:
                    # Ø¨Ø®Ø´ top candidates Ø¨Ø±Ø§ÛŒ rerank
                    rerank_top_n = min(200, len(df))
                    candidates = df.head(rerank_top_n)
                    # Ø³Ø§Ø®Øª Ø¬ÙØªâ€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ reranker: (query, candidate_text)
                    rerank_inputs = []
                    for i, row in candidates.iterrows():
                        # Ø¨Ø±Ø§ÛŒ rerank Ø§Ø² snippet (ÛŒØ§ title+snippet) Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
                        cand_text = (row["title"] + " . " + (row["snippet"] or ""))[:512]
                        rerank_inputs.append((user_query, cand_text))
                    # Ú¯Ø±ÙØªÙ† Ù†Ù…Ø±Ø§Øª reranker
                    try:
                        rerank_scores = reranker.predict(rerank_inputs, convert_to_numpy=True, show_progress_bar=False)
                        df.loc[:rerank_top_n-1, "rerank_score"] = rerank_scores
                        # Ú©Ø§Ù…Ù¾ÙˆØ²ÛŒØª Ù†Ù‡Ø§ÛŒÛŒ: ØªØ±Ú©ÛŒØ¨ cosine Ùˆ rerank (ÙˆØ²Ù† Ø¯Ù„Ø®ÙˆØ§Ù‡)
                        alpha = 0.5
                        # Normalize rerank scores to 0-1
                        if "rerank_score" in df.columns:
                            arr = df["rerank_score"].fillna(df["rerank_score"].min()).to_numpy()
                            if arr.max() - arr.min() > 1e-6:
                                arr_norm = (arr - arr.min()) / (arr.max() - arr.min())
                            else:
                                arr_norm = np.zeros_like(arr)
                            df["composite"] = alpha * df["similarity"] + (1 - alpha) * arr_norm
                            df = df.sort_values("composite", ascending=False).reset_index(drop=True)
                        else:
                            df = df.sort_values("similarity", ascending=False).reset_index(drop=True)
                    except Exception as e:
                        st.warning(f"Ø®Ø·Ø§ Ø¯Ø± reranker: {e} â€” Ù†ØªØ§ÛŒØ¬ Ø¨Ø¯ÙˆÙ† rerank Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯.")
                        df = df.sort_values("similarity", ascending=False).reset_index(drop=True)
                else:
                    df = df.sort_values("similarity", ascending=False).reset_index(drop=True)

                # 7) Ù†Ù…Ø§ÛŒØ´ Ù†ØªØ§ÛŒØ¬ Ø§ØµÙ„ÛŒ (top_k)
                st.markdown("## Ù†ØªØ§ÛŒØ¬: Ù…Ù‚Ø§Ù„Ø§Øª Ù…Ø´Ø§Ø¨Ù‡")
                top_df = df.head(top_k).copy()

                for idx, row in top_df.iterrows():
                    with st.expander(f"#{idx+1} â€” {row['title']}  (Ø´Ø¨Ø§Ù‡Øª: {row.get('composite', row['similarity']):.2f})"):
                        st.markdown(f"**Ø¹Ù†ÙˆØ§Ù†:** [{row['title']}]({row.get('url','')})")
                        st.markdown(f"**Ù†Ø´Ø±ÛŒÙ‡ / Ú©Ù†ÙØ±Ø§Ù†Ø³:** `{row.get('venue')}`  â€”  **ØªØ§Ø±ÛŒØ®:** `{row.get('publicationDate')}`")
                        st.markdown(f"**Ø®Ù„Ø§ØµÙ‡ / snippet:**\n\n{row.get('snippet')}")
                        st.markdown("**Ù†ÙˆÛŒØ³Ù†Ø¯Ú¯Ø§Ù†:**")
                        auths = row.get("authors", [])
                        if auths:
                            for a in auths:
                                name = a.get("name") or "Ù†Ø§Ø´Ù†Ø§Ø³"
                                aid = a.get("authorId")
                                if aid:
                                    st.markdown(f"- [{name}](https://www.semanticscholar.org/author/{aid})")
                                else:
                                    st.markdown(f"- {name}")
                        else:
                            st.markdown("- Ù†Ø§Ù…Ø´Ø®Øµ")

                # 8) Ø®Ù„Ø§ØµÙ‡ Ø¢Ù…Ø§Ø±ÛŒ Ùˆ Ù†Ù…ÙˆØ¯Ø§Ø± Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ù¾Ø±ÙˆÙ¾ÙˆØ²Ø§Ù„
                st.markdown("---")
                st.subheader("Ø®Ù„Ø§ØµÙ‡ Ùˆ Ù†Ù…ÙˆØ¯Ø§Ø± Ø´Ø¨Ø§Ù‡Øª")
                fig = px.bar(x=top_df["title"].apply(lambda t: t[:80]), y=top_df.get("composite", top_df["similarity"]),
                             labels={"x": "Ø¹Ù†ÙˆØ§Ù† (Ú©ÙˆØªØ§Ù‡â€ŒØ´Ø¯Ù‡)", "y": "Ø§Ù…ØªÛŒØ§Ø² Ø´Ø¨Ø§Ù‡Øª"}, height=380)
                st.plotly_chart(fig, use_container_width=True)

                # 9) Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†ÙˆÛŒØ³Ù†Ø¯Ú¯Ø§Ù† Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ: Ø´Ù…Ø§Ø±Ø´ Ù…Ù‚Ø§Ù„Ø§Øª Ù…Ø±ØªØ¨Ø· Ùˆ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† similarity
                st.markdown("---")
                st.subheader("Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù†ÙˆÛŒØ³Ù†Ø¯Ú¯Ø§Ù† (Ø§Ø² Ø±ÙˆÛŒ Ù…Ù‚Ø§Ù„Ø§Øª Ù…Ø´Ø§Ø¨Ù‡)")
                # Ø´Ù…Ø§Ø±Ø´ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆÛŒØ³Ù†Ø¯Ú¯Ø§Ù† Ø¯Ø± Ú©Ù„ Ù†ØªØ§ÛŒØ¬
                author_stats = {}  # id -> {name, count, avg_sim}
                for i, row in df.iterrows():
                    sim = float(row.get("composite", row["similarity"]))
                    for a in row.get("authors", []):
                        aid = a.get("authorId") or a.get("name")
                        name = a.get("name") or "Ù†Ø§Ø´Ù†Ø§Ø³"
                        if not aid:
                            continue
                        if aid not in author_stats:
                            author_stats[aid] = {"name": name, "count": 0, "sum_sim": 0.0}
                        author_stats[aid]["count"] += 1
                        author_stats[aid]["sum_sim"] += sim

                # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ DataFrame
                authors_list = []
                for aid, stt in author_stats.items():
                    avg_sim = stt["sum_sim"] / stt["count"] if stt["count"] else 0
                    authors_list.append({"authorId": aid, "name": stt["name"], "matched_papers": stt["count"], "avg_similarity": avg_sim})
                authors_df = pd.DataFrame(authors_list)
                if not authors_df.empty:
                    authors_df = authors_df.sort_values(["matched_papers", "avg_similarity"], ascending=[False, False]).reset_index(drop=True)
                    # Ù†Ù…Ø§ÛŒØ´ table Ø®Ù„Ø§ØµÙ‡
                    st.dataframe(authors_df.head(20))
                    # Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡ Ø¨Ø±ØªØ±ØŒ Ù„ÛŒÙ†Ú© Ø¨Ù‡ Ù¾Ø±ÙˆÙØ§ÛŒÙ„ Ùˆ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨ÛŒØ´ØªØ± Ø±Ø§ Ø¨ÛŒØ§ÙˆØ±
                    st.markdown("### Ø¬Ø²Ø¦ÛŒØ§Øª Ù†ÙˆÛŒØ³Ù†Ø¯Ú¯Ø§Ù† Ø¨Ø±ØªØ±")
                    for i, arow in authors_df.head(10).iterrows():
                        aid = arow["authorId"]
                        name = arow["name"]
                        col1, col2 = st.columns([3, 1])
                        with col1:
                            st.markdown(f"**[{name}](https://www.semanticscholar.org/author/{aid})** â€” ØªØ¹Ø¯Ø§Ø¯ Ù…Ù‚Ø§Ù„Ø§Øª Ù…Ø·Ø§Ø¨Ù‚: `{int(arow['matched_papers'])}`, Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø´Ø¨Ø§Ù‡Øª: `{arow['avg_similarity']:.2f}`")
                        with col2:
                            if st.button(f"Ø¬Ø²Ø¦ÛŒØ§Øª {i+1}", key=f"author_detail_{i}"):
                                prof = get_author_profile(aid)
                                if prof:
                                    st.write(prof)
                                else:
                                    st.write("Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨ÛŒØ´ØªØ± Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª.")

                else:
                    st.info("Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡â€ŒØ§ÛŒ Ø§Ø² Ù†ØªØ§ÛŒØ¬ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø´Ø¯.")

                # 10) Ù†Ú©Ø§Øª Ù‚Ø§Ø¨Ù„ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ø± Ù¾Ø±ÙˆÙ¾ÙˆØ²Ø§Ù„
                st.markdown("---")
                st.subheader("Ù†Ú©Ø§ØªÛŒ Ú©Ù‡ Ø¯Ø± Ù¾Ø±ÙˆÙ¾ÙˆØ²Ø§Ù„ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯ (Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯)")
                st.markdown(
                    """
- Ø§ÛŒÙ† Ø¯Ù…ÙˆÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² **Sentence Transformers** Ùˆ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹Ù…ÙˆÙ…ÛŒ Semantic Scholar Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡ Ø§Ø³Øª.
- Ø¯Ø± Ù†Ø³Ø®Ù‡â€ŒÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ø² **Scopus API** (Ø¨Ø§ Ø¯Ø³ØªØ±Ø³ÛŒ Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡) ØªØ§Ù…ÛŒÙ† Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ú©Ù‡ Ù¾ÙˆØ´Ø´ Ùˆ Ø¯Ù‚Øª Ø­ÙˆØ²Ù‡â€ŒØ§ÛŒ Ø¨Ø§Ù„Ø§ØªØ±ÛŒ ÙØ±Ø§Ù‡Ù… Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
- Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù†Ù‡Ø§ÛŒÛŒ Ø´Ø§Ù…Ù„: (1) dense retrieval Ø¨Ø§ Ù…Ø¯Ù„ Ø¹Ù„Ù…ÛŒ (SPECTER ÛŒØ§ Ù…Ø¯Ù„ fine-tuned) + (2) cross-encoder reranker + (3) fusion Ø¨Ø§ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ÛŒÚ¯Ø§Ù†ÛŒ (h-index, recency, conflict-of-interest).
- Ø¨Ø±Ø§ÛŒ Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ù‚Øª Ø¯Ø± Ø­ÙˆØ²Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ®ØµØµÛŒ (Ù…Ø«Ù„Ø§Ù‹ Ø¯Ø§Ø±ÙˆØ³Ø§Ø²ÛŒ) Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ù…Ø¯Ù„ Ø±ÙˆÛŒ Ø¯ÛŒØªØ§Ø³Øª domain-specific fine-tune Ø´ÙˆØ¯.
"""
                )

                st.success("ğŸ¯ ØªÙ…Ø§Ù… Ø´Ø¯ â€” Ø§ÛŒÙ† Ù†Ø³Ø®Ù‡ Ø¯Ù…ÙˆÛŒ Ù¾Ø±ÙˆÙ¾ÙˆØ²Ø§Ù„ Ø§Ø³Øª. Ø¨Ø±Ø§ÛŒ Ù†Ø³Ø®Ù‡â€ŒÛŒ Ù†Ù‡Ø§ÛŒÛŒØŒ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ§Ù… pipeline Ú©Ø§Ù…Ù„ Scopus-based Ùˆ fine-tuning Ø±Ø§ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†Ù….")
